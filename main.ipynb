{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04964bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dolma-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import fasttext\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dolma import BaseTagger, add_tagger\n",
    "from dolma.core.data_types import DocResult, Document, Span\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "OUTPUT_TOKENIZED_DIR = Path(\"data/output/tokenized_c4\")\n",
    "OUTPUT_TOKENIZED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROGRESS_PATH = Path(\"data/state/c4_progress.json\")\n",
    "PROGRESS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = \"utils/autotuned_fasttext_model.bin\"\n",
    "LOCAL_C4_FOLDER = Path(\"/Users/kaionamartinson/Desktop/Cultural-Analytics/dolma/c4\")\n",
    "\n",
    "\n",
    "SHARD_SIZE = 500_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0276de8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText model loaded successfully from utils/autotuned_fasttext_model.bin\n"
     ]
    }
   ],
   "source": [
    "model_path = \"utils/autotuned_fasttext_model.bin\"\n",
    "\n",
    "try:\n",
    "    # Load the FastText model\n",
    "    model = fasttext.load_model(model_path)\n",
    "    print(f\"FastText model loaded successfully from {model_path}\")\n",
    "except ValueError as e:\n",
    "    print(\n",
    "        f\"Error loading model: {e}. It might be that the file is corrupted or not a valid FastText model.\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file not found at {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab886c",
   "metadata": {},
   "source": [
    "### Utility Functions & Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40ee505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 66 AAPI keywords\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "AAPI_KEYWORDS_PATH = Path(\n",
    "    \"/Users/kaionamartinson/Desktop/Cultural-Analytics/Cultural-Analytics-AAPI/data/aapiGroups.pkl\"\n",
    ")\n",
    "\n",
    "with AAPI_KEYWORDS_PATH.open(\"rb\") as f:\n",
    "    AAPI_KEYWORDS = pickle.load(f)\n",
    "\n",
    "# Safety check\n",
    "if not isinstance(AAPI_KEYWORDS, set):\n",
    "    AAPI_KEYWORDS = set(AAPI_KEYWORDS)\n",
    "\n",
    "print(f\"Loaded {len(AAPI_KEYWORDS)} AAPI keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce51c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    try:\n",
    "        # Load the FastText model\n",
    "        model = fasttext.load_model(MODEL_PATH)\n",
    "        return model\n",
    "        print(f\"FastText model loaded successfully from {MODEL_PATH}\")\n",
    "    except ValueError as e:\n",
    "        print(\n",
    "            f\"Error loading model: {e}. It might be that the file is corrupted or not a valid FastText model.\"\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {MODEL_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e3d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_here() -> Path:\n",
    "    \"\"\"Return a reasonable 'here' directory, even in notebooks/REPL.\"\"\"\n",
    "    if \"__file__\" in globals():\n",
    "        return Path(__file__).resolve()\n",
    "    return Path.cwd()\n",
    "\n",
    "\n",
    "def _find_pickle(explicit: Optional[str]) -> Path:\n",
    "    \"\"\"\n",
    "    Find the aapiGroups.pkl file.\n",
    "\n",
    "    Search order:\n",
    "      1. Explicit path argument\n",
    "      2. Env var: AAPI_KEYWORDS_PICKLE\n",
    "      3. Common repo-relative locations\n",
    "    \"\"\"\n",
    "    tried: List[str] = []\n",
    "\n",
    "    def _check(path: Path) -> Optional[Path]:\n",
    "        tried.append(str(path))\n",
    "        if path.exists():\n",
    "            return path.resolve()\n",
    "        return None\n",
    "\n",
    "    if explicit:\n",
    "        p = Path(explicit).expanduser().resolve()\n",
    "        found = _check(p)\n",
    "        if found:\n",
    "            return found\n",
    "\n",
    "    env_path = os.environ.get(\"AAPI_KEYWORDS_PICKLE\")\n",
    "    if env_path:\n",
    "        p = Path(env_path).expanduser().resolve()\n",
    "        found = _check(p)\n",
    "        if found:\n",
    "            return found\n",
    "\n",
    "    here = _safe_here()\n",
    "    repo_root = here.parents[2] if len(here.parents) >= 3 else here\n",
    "\n",
    "    candidates = [\n",
    "        repo_root / \"data\" / \"aapiGroups.pkl\",\n",
    "        here.parent / \"data\" / \"aapiGroups.pkl\",\n",
    "        repo_root / \"utils\" / \"data\" / \"aapiGroups.pkl\",\n",
    "        Path.cwd() / \"data\" / \"aapiGroups.pkl\",\n",
    "    ]\n",
    "\n",
    "    for cand in candidates:\n",
    "        found = _check(cand)\n",
    "        if found:\n",
    "            return found\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate aapiGroups.pkl. Searched:\\n  - \"\n",
    "        + \"\\n  - \".join(tried)\n",
    "        + \"\\nTip: set env var AAPI_KEYWORDS_PICKLE=/abs/path/to/aapiGroups.pkl \"\n",
    "        \"or pass keyword_pickle=... when constructing AAPIKeywordsTagger.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b3ae9",
   "metadata": {},
   "source": [
    "### Dolma Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19019dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_tagger(\"aapi_keywords_v1\")\n",
    "class AAPIKeywordsTagger(BaseTagger):\n",
    "    \"\"\"\n",
    "    Tags documents that contain any AAPI-related keyword loaded from a pickle.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keyword_pickle: Optional[str] = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.keyword_pickle = _find_pickle(keyword_pickle)\n",
    "\n",
    "        with self.keyword_pickle.open(\"rb\") as f:\n",
    "            raw_terms = pickle.load(f)\n",
    "\n",
    "        terms = [str(t).strip().lower() for t in list(raw_terms)]\n",
    "        if not terms:\n",
    "            raise ValueError(f\"No terms found in {self.keyword_pickle}\")\n",
    "\n",
    "        pattern = r\"\\b(\" + \"|\".join(re.escape(t) for t in terms) + r\")\\b\"\n",
    "        self.regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "    def predict(self, doc: Document) -> DocResult:\n",
    "        text = doc.text or \"\"\n",
    "        matches = self.regex.findall(text)\n",
    "        if not matches:\n",
    "            # no matches → score 0\n",
    "            span = Span(start=0, end=0, type=\"aapi_keyword\", score=0.0)\n",
    "            return DocResult(doc=doc, spans=[span])\n",
    "\n",
    "        # unique matches → score is count of unique AAPI terms present\n",
    "        unique_matches = {m.lower() for m in matches}\n",
    "        score = float(len(unique_matches))\n",
    "\n",
    "        span = Span(\n",
    "            start=0,\n",
    "            end=len(text),\n",
    "            type=\"aapi_keyword\",\n",
    "            score=score,\n",
    "        )\n",
    "        return DocResult(doc=doc, spans=[span])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14210657",
   "metadata": {},
   "source": [
    "### Dolma Mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04aa1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_aapi_doc(result: DocResult) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Given a DocResult from AAPIKeywordsTagger, return a JSON-serializable dict\n",
    "    if score > 0, else return None (filter out the doc).\n",
    "    \"\"\"\n",
    "    doc = result.doc\n",
    "    spans = result.spans or []\n",
    "\n",
    "    score = 0.0\n",
    "    if spans:\n",
    "        score = float(spans[0].score or 0.0)\n",
    "\n",
    "    if score <= 0.0:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"id\": doc.id,\n",
    "        \"text\": doc.text,\n",
    "        # \"source\": getattr(doc, \"source\", None),\n",
    "        # \"aapi_score\": score,\n",
    "        # \"aapi_spans\": [\n",
    "        #     {\n",
    "        #         \"start\": s.start,\n",
    "        #         \"end\": s.end,\n",
    "        #         \"type\": s.type,\n",
    "        #     }\n",
    "        #     for s in spans\n",
    "        # ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07cc25",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bdad9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAPITokenizer:\n",
    "\n",
    "    def __init__(self, keyword_pickle: Optional[str] = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.keyword_pickle = _find_pickle(keyword_pickle)\n",
    "        with self.keyword_pickle.open(\"rb\") as f:\n",
    "            raw_terms = pickle.load(f)\n",
    "        terms = [str(t).strip().lower() for t in list(raw_terms)]\n",
    "        if not terms:\n",
    "            raise ValueError(f\"No terms found in {self.keyword_pickle}\")\n",
    "\n",
    "        self.aapi_groups_set = set(terms)\n",
    "\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        for token_text in self.aapi_groups_set:\n",
    "            self.nlp.tokenizer.add_special_case(token_text, [{\"ORTH\": token_text}])\n",
    "\n",
    "    def tokenize(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        return self.nlp(data[\"text\"])  # .to_json() changed this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5730ef",
   "metadata": {},
   "source": [
    "### Process Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e35467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(count: int) -> None:\n",
    "    \"\"\"\n",
    "    Save how many C4 docs we've fully traversed/processed (for resume).\n",
    "    \"\"\"\n",
    "    PROGRESS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with PROGRESS_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"c4_docs_done\": count}, f)\n",
    "\n",
    "\n",
    "def load_progress() -> int:\n",
    "    \"\"\"\n",
    "    Load how many C4 docs we've previously processed. Returns 0 if none.\n",
    "    \"\"\"\n",
    "    if PROGRESS_PATH.exists():\n",
    "        with PROGRESS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return int(data.get(\"c4_docs_done\", 0))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def open_new_shard(out_dir: Path, shard_idx: int) -> gzip.GzipFile:\n",
    "    \"\"\"\n",
    "    Open a new gzip'd JSONL shard for writing mixed docs.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    shard_path = out_dir / f\"mixed.{shard_idx:09d}.jsonl.gz\"\n",
    "    return gzip.open(shard_path, \"wt\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ea301",
   "metadata": {},
   "source": [
    "### Verbs & Adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a44b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_group_lexicon(groups):\n",
    "    \"\"\"\n",
    "    Map surface forms -> canonical ethnicity.\n",
    "    Handles simple plurals like 'filipinos' -> 'filipino'.\n",
    "    \"\"\"\n",
    "    lex = {}\n",
    "    for g in groups:\n",
    "        g_l = g.lower()\n",
    "        lex[g_l] = g_l\n",
    "        lex[g_l + \"s\"] = g_l  # naive plural\n",
    "    return lex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622c4ef",
   "metadata": {},
   "source": [
    "Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13e35222",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_ADJ = {\"north\", \"south\", \"east\", \"west\", \"central\"}\n",
    "\n",
    "\n",
    "def is_directional(adj_lemma: str) -> bool:\n",
    "    return adj_lemma in STOP_ADJ or any(s in adj_lemma for s in STOP_ADJ)\n",
    "\n",
    "\n",
    "def get_ethnicity_cache(sentence, group_lexicon):\n",
    "    \"\"\"\n",
    "    Precompute ethnicity tokens in the sentence.\n",
    "    Returns:\n",
    "      - eth_by_i: token index -> group\n",
    "      - groups_in_sentence: set of group strings\n",
    "    \"\"\"\n",
    "    eth_by_i = {}\n",
    "    groups_in_sentence = set()\n",
    "    for tok in sentence:\n",
    "        form = tok.text.lower()\n",
    "        eth = group_lexicon.get(form)\n",
    "        if eth:\n",
    "            eth_by_i[tok.i] = eth\n",
    "            groups_in_sentence.add(eth)\n",
    "    return eth_by_i, groups_in_sentence\n",
    "\n",
    "\n",
    "def get_subjects_for_pred(pred):\n",
    "    \"\"\"\n",
    "    Find subjects (nsubj/nsubjpass) associated with this predicate\n",
    "    (either the ADJ itself or its governing AUX/VERB).\n",
    "    \"\"\"\n",
    "    subs = [c for c in pred.children if c.dep_ in {\"nsubj\", \"nsubjpass\"}]\n",
    "    if subs:\n",
    "        return subs\n",
    "\n",
    "    head = pred.head\n",
    "    visited = set()\n",
    "    while head is not None and head not in visited:\n",
    "        visited.add(head)\n",
    "        subs = [c for c in head.children if c.dep_ in {\"nsubj\", \"nsubjpass\"}]\n",
    "        if subs:\n",
    "            return subs\n",
    "        if head.dep_ in {\"conj\", \"acomp\", \"attr\"} or head.pos_ == \"AUX\":\n",
    "            head = head.head\n",
    "        else:\n",
    "            break\n",
    "    return []\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Main collector\n",
    "# -------------------------\n",
    "def collect_adj(sentence, group_lexicon, ethnicity_dict):\n",
    "    # Precompute ethnicity info for this sentence\n",
    "    eth_by_i, groups_in_sentence = get_ethnicity_cache(sentence, group_lexicon)\n",
    "    if not groups_in_sentence:\n",
    "        return  # no relevant groups, skip\n",
    "\n",
    "    for adj in sentence:\n",
    "        if adj.pos_ != \"ADJ\":\n",
    "            continue\n",
    "\n",
    "        lemma = adj.lemma_.lower()\n",
    "        if not lemma or len(lemma) <= 2:\n",
    "            continue\n",
    "        if is_directional(lemma):\n",
    "            continue\n",
    "        if adj.ent_type_ == \"NORP\":  # don't count ethnicity adjectives themselves\n",
    "            continue\n",
    "\n",
    "        subjects = get_subjects_for_pred(adj)\n",
    "        attached = False\n",
    "\n",
    "        # 1) Attach via subject subtrees\n",
    "        for subj_tok in subjects:\n",
    "            for tok in subj_tok.subtree:\n",
    "                eth = eth_by_i.get(tok.i)\n",
    "                if eth is None:\n",
    "                    continue\n",
    "                counter = ethnicity_dict.get(eth)\n",
    "                if counter is None:\n",
    "                    counter = ethnicity_dict[eth] = Counter()\n",
    "                counter[lemma] += 1\n",
    "                attached = True\n",
    "\n",
    "        # 2) Fallback: no subject found, but groups exist in sentence\n",
    "        # Useful for weird fragments like \"Filipinos hardworking.\"\n",
    "        if not attached:\n",
    "            for eth in groups_in_sentence:\n",
    "                counter = ethnicity_dict.get(eth)\n",
    "                if counter is None:\n",
    "                    counter = ethnicity_dict[eth] = Counter()\n",
    "                counter[lemma] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631da56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76d280e6",
   "metadata": {},
   "source": [
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c81fcbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjects_for_pred(pred):\n",
    "    subs = [c for c in pred.children if c.dep_ in {\"nsubj\", \"nsubjpass\"}]\n",
    "    if subs:\n",
    "        return subs\n",
    "\n",
    "    head = pred.head\n",
    "    visited = set()\n",
    "    while head is not None and head not in visited:\n",
    "        visited.add(head)\n",
    "        subs = [c for c in head.children if c.dep_ in {\"nsubj\", \"nsubjpass\"}]\n",
    "        if subs:\n",
    "            return subs\n",
    "        if head.dep_ in {\"conj\", \"acomp\", \"attr\"} or head.pos_ == \"AUX\":\n",
    "            head = head.head\n",
    "        else:\n",
    "            break\n",
    "    return []\n",
    "\n",
    "\n",
    "BAD_VERBS = {\"be\", \"have\", \"do\"}\n",
    "BAD_LEMMAS = {\"orient\"}  # add more if needed\n",
    "\n",
    "\n",
    "def collect_verb(sentence, group_lexicon, ethnicity_dict, use_fallback=False):\n",
    "    eth_by_i = {}\n",
    "    groups_in_sentence = set()\n",
    "\n",
    "    for tok in sentence:\n",
    "        eth = group_lexicon.get(tok.text.lower())\n",
    "        if eth:\n",
    "            eth_by_i[tok.i] = eth\n",
    "            groups_in_sentence.add(eth)\n",
    "\n",
    "    if not groups_in_sentence:\n",
    "        return\n",
    "\n",
    "    for token in sentence:\n",
    "        if token.pos_ not in {\"VERB\", \"AUX\"}:\n",
    "            continue\n",
    "\n",
    "        lemma = token.lemma_.lower()\n",
    "\n",
    "        if lemma in BAD_VERBS or lemma in BAD_LEMMAS:\n",
    "            continue\n",
    "\n",
    "        if token.tag_ not in {\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"}:\n",
    "            continue\n",
    "\n",
    "        # ---- NEW: relative clause verbs ----\n",
    "        if token.dep_ == \"relcl\":\n",
    "            head = token.head\n",
    "            for tok in head.subtree:\n",
    "                eth = group_lexicon.get(tok.text.lower())\n",
    "                if eth:\n",
    "                    ethnicity_dict.setdefault(eth, Counter())[lemma] += 1\n",
    "            continue\n",
    "\n",
    "        # ---- NEW: participle / reduced clause verbs ----\n",
    "        if token.dep_ in {\"acl\", \"acl:relcl\"}:\n",
    "            head = token.head\n",
    "            for tok in head.subtree:\n",
    "                eth = group_lexicon.get(tok.text.lower())\n",
    "                if eth:\n",
    "                    ethnicity_dict.setdefault(eth, Counter())[lemma] += 1\n",
    "            continue\n",
    "\n",
    "        subjects = get_subjects_for_pred(token)\n",
    "        attached = False\n",
    "\n",
    "        for subj in subjects:\n",
    "            for tok in subj.subtree:\n",
    "                eth = eth_by_i.get(tok.i)\n",
    "                if eth:\n",
    "                    ethnicity_dict.setdefault(eth, Counter())[lemma] += 1\n",
    "                    attached = True\n",
    "\n",
    "        if not attached and use_fallback and len(groups_in_sentence) == 1:\n",
    "            eth = next(iter(groups_in_sentence))\n",
    "            ethnicity_dict.setdefault(eth, Counter())[lemma] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f16ae2",
   "metadata": {},
   "source": [
    "### Windows and nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e58da1",
   "metadata": {},
   "source": [
    "window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a78ca082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_mask_sentence(ethnicity_term, sent_tokens, tokens_lower, window=5):\n",
    "    \"\"\"\n",
    "    sent_tokens: list of token strings (in order)\n",
    "    aapi_groups_set: set of lowercase group terms\n",
    "    window: number of tokens to keep on each side\n",
    "\n",
    "    Returns: a masked window string or None if no ethnicity term found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find its index in the ordered list\n",
    "    idx = tokens_lower.index(ethnicity_term)\n",
    "\n",
    "    # Mask token\n",
    "    tokens_masked = sent_tokens[:]  # copy list\n",
    "    tokens_masked[idx] = \"[ETHNICITY]\"\n",
    "\n",
    "    # Window boundaries\n",
    "    start = max(0, idx - window)\n",
    "    end = min(len(tokens_masked), idx + window + 1)\n",
    "    # final join\n",
    "    return \" \".join(tokens_masked[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ed706",
   "metadata": {},
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "739f2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethnicity_modified_nouns(sentence, aapi_groups_set=None):\n",
    "    \"\"\"\n",
    "    Find nouns modified by an ethnicity term.\n",
    "    Returns: list of (ethnicity, noun) pairs.\n",
    "\n",
    "    aapi_groups_set: optional set of lowercase ethnicity strings to restrict to.\n",
    "                     If None, we use spaCy's NORP tag only.\n",
    "    \"\"\"\n",
    "    for token in sentence:\n",
    "        # we want the head noun: \"girls\", \"women\", \"doctors\"\n",
    "        if token.pos_ != \"NOUN\":\n",
    "            continue\n",
    "\n",
    "        noun_lemma = token.lemma_.lower()\n",
    "\n",
    "        for child in token.children:\n",
    "            # modifier must be attached to the noun\n",
    "            if child.dep_ not in {\"amod\", \"compound\"}:\n",
    "                continue\n",
    "\n",
    "            child_lower = child.text.lower()\n",
    "\n",
    "            if not (child.ent_type_ == \"NORP\") or (\n",
    "                aapi_groups_set is not None and child_lower in aapi_groups_set\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            return noun_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc2382",
   "metadata": {},
   "source": [
    "### Actual Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49ad8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e384348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__0',), array([1.00000882]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model()\n",
    "\n",
    "model.predict(\"new cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2a1fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_artifacts_safely(\n",
    "    noun_heads_counter, aapi_counter_pass, verb_ethnicity_dict, adj_ethnicity_dict\n",
    "):\n",
    "    \"\"\"\n",
    "    Safely save artifacts so crashes don't corrupt your file.\n",
    "    Uses atomic replace: either old file or new file, never half-written.\n",
    "    \"\"\"\n",
    "\n",
    "    data = {\n",
    "        \"noun_heads_counter\": noun_heads_counter,\n",
    "        \"aapi_counter_pass\": aapi_counter_pass,\n",
    "        \"verb_ethnicity_dict\": verb_ethnicity_dict,\n",
    "        \"adj_ethnicity_dict\": adj_ethnicity_dict,\n",
    "    }\n",
    "\n",
    "    tmp_path = \"ethnicity_artifacts.pkl.tmp\"\n",
    "    final_path = \"ethnicity_artifacts.pkl\"\n",
    "\n",
    "    # Write to a temp file first\n",
    "    with open(tmp_path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "        f.flush()\n",
    "        os.fsync(f.fileno())  # force write to disk\n",
    "\n",
    "    # Atomic replace: either old file or the new one, never half-written\n",
    "    os.replace(tmp_path, final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d973ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ARTIFACTS_PATH = Path(\"ethnicity_artifacts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4c4c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def iter_local_c4_files(folder_path):\n",
    "    \"\"\"get c4 and it\"\"\"\n",
    "    folder_path = Path(folder_path)\n",
    "\n",
    "    for gz_file in sorted(folder_path.glob(\"*.json.gz\")):\n",
    "        print(f\"Reading: {gz_file}\")\n",
    "\n",
    "        with gzip.open(gz_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    yield data\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping bad line: {e}\")\n",
    "                    continue\n",
    "            print(\"ending reading this file, no more lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a5eb077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e2d0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loop(\n",
    "    noun_heads_counter,\n",
    "    aapi_counter_pass,\n",
    "    verb_ethnicity_dict,\n",
    "    adj_ethnicity_dict,\n",
    "    out_dirname: Path = OUTPUT_TOKENIZED_DIR,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Stream Dolma C4 docs, tag them with AAPIKeywordsTagger, filter them via\n",
    "    mix_aapi_doc, and write the kept ones into sharded .jsonl.gz files.\n",
    "\n",
    "    Resumable via PROGRESS_PATH (counts how many C4 docs have been processed).\n",
    "    \"\"\"\n",
    "    traversed = 0\n",
    "\n",
    "    tagger = AAPIKeywordsTagger()\n",
    "    tokenizer = AAPITokenizer()\n",
    "\n",
    "    ds = iter_local_c4_files(LOCAL_C4_FOLDER)\n",
    "\n",
    "    pbar = tqdm(\n",
    "        total=None,\n",
    "        desc=\"Processing C4 docs\",\n",
    "        mininterval=0.5,\n",
    "        dynamic_ncols=True,\n",
    "        leave=True,\n",
    "    )\n",
    "\n",
    "    docs_in_shard = 0\n",
    "\n",
    "    for data in ds:\n",
    "        pbar.update(1)\n",
    "        doc = Document(\n",
    "            id=data[\"id\"],\n",
    "            text=data[\"text\"],\n",
    "            source=data.get(\"source\"),\n",
    "        )\n",
    "\n",
    "        tagged = tagger.predict(doc)\n",
    "        mixed = mix_aapi_doc(tagged)\n",
    "\n",
    "        if mixed is not None:\n",
    "            tokenized = tokenizer.tokenize(mixed)\n",
    "\n",
    "            for sentence in tokenized.sents:\n",
    "                # tokenize and lower\n",
    "                tokens = [t.text for t in sentence]\n",
    "                tokens_lower = [t.lower() for t in tokens]\n",
    "\n",
    "                # checking if ethnicity is in sentence and taking those ethnicity if true\n",
    "                overlap = set(tokens_lower) & AAPI_KEYWORDS\n",
    "                if not overlap:\n",
    "                    continue  # skip if no ethnicity term\n",
    "                overlap_eth = list(overlap)[0]\n",
    "\n",
    "                # some collection before filtering\n",
    "                noun_heads_counter[ethnicity_modified_nouns(sentence, overlap)] += 1\n",
    "                aapi_counter_pass[overlap_eth] += 1\n",
    "\n",
    "                # running through my model\n",
    "                window_text = (\n",
    "                    window_mask_sentence(overlap_eth, tokens, tokens_lower)\n",
    "                    .replace(\"\\n\", \" \")\n",
    "                    .strip()\n",
    "                )\n",
    "                if model.predict(window_text)[0][0] == \"__label__0\":\n",
    "                    continue\n",
    "\n",
    "                aapi_counter_pass[\n",
    "                    overlap_eth\n",
    "                ] += 1  # stating that the ethnicity passes model\n",
    "\n",
    "                lex = build_group_lexicon(overlap)\n",
    "                collect_verb(sentence, lex, verb_ethnicity_dict)\n",
    "                collect_adj(sentence, lex, adj_ethnicity_dict)\n",
    "\n",
    "            if docs_in_shard % 1000 == 0:\n",
    "                save_artifacts_safely(\n",
    "                    noun_heads_counter,\n",
    "                    aapi_counter_pass,\n",
    "                    verb_ethnicity_dict,\n",
    "                    adj_ethnicity_dict,\n",
    "                )\n",
    "\n",
    "            docs_in_shard += 1\n",
    "\n",
    "            traversed += 1\n",
    "            if traversed % 1000 == 0:\n",
    "                save_progress(traversed)\n",
    "\n",
    "    pbar.close()\n",
    "    save_artifacts_safely(\n",
    "        noun_heads_counter, aapi_counter_pass, verb_ethnicity_dict, adj_ethnicity_dict\n",
    "    )\n",
    "\n",
    "    save_progress(traversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fff62d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_heads_counter = Counter()\n",
    "aapi_counter_pass = Counter()\n",
    "verb_ethnicity_dict = {}\n",
    "adj_ethnicity_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd23b43",
   "metadata": {},
   "source": [
    "### Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c0092f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: /Users/kaionamartinson/Desktop/Cultural-Analytics/dolma/c4/c4-0000.json.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs: 1551317it [3:14:24, 98.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending reading this file, no more lines\n",
      "Reading: /Users/kaionamartinson/Desktop/Cultural-Analytics/dolma/c4/c4-0001.json.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs: 1553843it [3:14:51, 101.24it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoun_heads_counter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maapi_counter_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverb_ethnicity_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_ethnicity_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mrun_loop\u001b[39m\u001b[34m(noun_heads_counter, aapi_counter_pass, verb_ethnicity_dict, adj_ethnicity_dict, out_dirname)\u001b[39m\n\u001b[32m     40\u001b[39m mixed = mix_aapi_doc(tagged)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mixed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     tokenized = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tokenized.sents:\n\u001b[32m     46\u001b[39m         \u001b[38;5;66;03m# tokenize and lower\u001b[39;00m\n\u001b[32m     47\u001b[39m         tokens = [t.text \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m sentence]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mAAPITokenizer.tokenize\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dolma-env/lib/python3.11/site-packages/spacy/language.py:1049\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1047\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1051\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1052\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs: 1553941it [3:15:08, 101.24it/s]"
     ]
    }
   ],
   "source": [
    "run_loop(noun_heads_counter, aapi_counter_pass, verb_ethnicity_dict, adj_ethnicity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2323d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"ethnicity_artifacts.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# unpack\n",
    "noun_heads_counter = data[\"noun_heads_counter\"]\n",
    "aapi_counter_pass = data[\"aapi_counter_pass\"]\n",
    "verb_ethnicity_dict = data[\"verb_ethnicity_dict\"]\n",
    "adj_ethnicity_dict = data[\"adj_ethnicity_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4cde31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'chinese': 2})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "adj_ethnicity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8e2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({None: 1091,\n",
       "         'community': 4,\n",
       "         'agreement': 3,\n",
       "         'dish': 2,\n",
       "         'official': 2,\n",
       "         'tribe': 2,\n",
       "         'immigrant': 2,\n",
       "         'rule': 2,\n",
       "         'newspaper': 2,\n",
       "         'delegation': 2,\n",
       "         'culture': 1,\n",
       "         'center': 1,\n",
       "         'attorney': 1,\n",
       "         'population': 1,\n",
       "         'wing': 1,\n",
       "         '~.': 1,\n",
       "         'audience': 1,\n",
       "         'ally': 1,\n",
       "         'card': 1,\n",
       "         'phalai': 1,\n",
       "         'baht': 1,\n",
       "         'petroglyph': 1,\n",
       "         'frontier': 1,\n",
       "         'bandit': 1,\n",
       "         'symbol': 1,\n",
       "         'citizen': 1,\n",
       "         'weave': 1,\n",
       "         'tourist': 1,\n",
       "         'airline': 1,\n",
       "         'capital': 1,\n",
       "         'emperor': 1,\n",
       "         'front': 1,\n",
       "         'corporation': 1,\n",
       "         'government': 1,\n",
       "         'uranium': 1,\n",
       "         'city': 1,\n",
       "         'man': 1,\n",
       "         'army': 1,\n",
       "         'country': 1,\n",
       "         'church': 1,\n",
       "         'principal': 1,\n",
       "         'girl': 1,\n",
       "         'dynasty': 1,\n",
       "         'coast': 1,\n",
       "         'vessel': 1,\n",
       "         'coffer': 1,\n",
       "         'company': 1,\n",
       "         'won': 1,\n",
       "         'cοlοny': 1,\n",
       "         'music': 1,\n",
       "         'soil': 1,\n",
       "         'side': 1,\n",
       "         'car': 1,\n",
       "         'hub': 1,\n",
       "         'trade': 1,\n",
       "         'district': 1,\n",
       "         'student': 1,\n",
       "         'pot': 1,\n",
       "         'blender': 1,\n",
       "         'building': 1,\n",
       "         'pirate': 1,\n",
       "         'school': 1,\n",
       "         'son': 1,\n",
       "         'viper': 1,\n",
       "         'leader': 1,\n",
       "         'grocer': 1,\n",
       "         'surgeon': 1,\n",
       "         'people': 1,\n",
       "         'language': 1,\n",
       "         'good': 1,\n",
       "         'architecture': 1,\n",
       "         'player': 1,\n",
       "         'ware': 1,\n",
       "         'map': 1,\n",
       "         'migrant': 1,\n",
       "         'technique': 1,\n",
       "         'upheaval': 1,\n",
       "         'exchange': 1,\n",
       "         'accord': 1,\n",
       "         'water': 1,\n",
       "         'influence': 1,\n",
       "         'cuisine': 1,\n",
       "         'artist': 1,\n",
       "         'branch': 1,\n",
       "         'theater': 1,\n",
       "         'business': 1,\n",
       "         'farmer': 1})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_heads_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa29fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4e34fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolma-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
