{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04964bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dolma-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dolma import BaseTagger, add_tagger\n",
    "from dolma.core.data_types import DocResult, Document, Span\n",
    "\n",
    "\n",
    "OUTPUT_MIXED_DIR = Path(\"data/output/mixed\")\n",
    "OUTPUT_MIXED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROGRESS_PATH = Path(\"data/state/c4_progress.json\")\n",
    "PROGRESS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SHARD_SIZE = 50_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab886c",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e3d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_here() -> Path:\n",
    "    \"\"\"Return a reasonable 'here' directory, even in notebooks/REPL.\"\"\"\n",
    "    if \"__file__\" in globals():\n",
    "        return Path(__file__).resolve()\n",
    "    return Path.cwd()\n",
    "\n",
    "\n",
    "def _find_pickle(explicit: Optional[str]) -> Path:\n",
    "    \"\"\"\n",
    "    Find the aapiGroups.pkl file.\n",
    "\n",
    "    Search order:\n",
    "      1. Explicit path argument\n",
    "      2. Env var: AAPI_KEYWORDS_PICKLE\n",
    "      3. Common repo-relative locations\n",
    "    \"\"\"\n",
    "    tried: List[str] = []\n",
    "\n",
    "    def _check(path: Path) -> Optional[Path]:\n",
    "        tried.append(str(path))\n",
    "        if path.exists():\n",
    "            return path.resolve()\n",
    "        return None\n",
    "\n",
    "    if explicit:\n",
    "        p = Path(explicit).expanduser().resolve()\n",
    "        found = _check(p)\n",
    "        if found:\n",
    "            return found\n",
    "\n",
    "    env_path = os.environ.get(\"AAPI_KEYWORDS_PICKLE\")\n",
    "    if env_path:\n",
    "        p = Path(env_path).expanduser().resolve()\n",
    "        found = _check(p)\n",
    "        if found:\n",
    "            return found\n",
    "\n",
    "    here = _safe_here()\n",
    "    repo_root = here.parents[2] if len(here.parents) >= 3 else here\n",
    "\n",
    "    candidates = [\n",
    "        repo_root / \"data\" / \"aapiGroups.pkl\",\n",
    "        here.parent / \"data\" / \"aapiGroups.pkl\",\n",
    "        repo_root / \"utils\" / \"data\" / \"aapiGroups.pkl\",\n",
    "        Path.cwd() / \"data\" / \"aapiGroups.pkl\",\n",
    "    ]\n",
    "\n",
    "    for cand in candidates:\n",
    "        found = _check(cand)\n",
    "        if found:\n",
    "            return found\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate aapiGroups.pkl. Searched:\\n  - \"\n",
    "        + \"\\n  - \".join(tried)\n",
    "        + \"\\nTip: set env var AAPI_KEYWORDS_PICKLE=/abs/path/to/aapiGroups.pkl \"\n",
    "        \"or pass keyword_pickle=... when constructing AAPIKeywordsTagger.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b3ae9",
   "metadata": {},
   "source": [
    "### Dolma Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19019dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_tagger(\"aapi_keywords_v1\")\n",
    "class AAPIKeywordsTagger(BaseTagger):\n",
    "    \"\"\"\n",
    "    Tags documents that contain any AAPI-related keyword loaded from a pickle.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keyword_pickle: Optional[str] = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.keyword_pickle = _find_pickle(keyword_pickle)\n",
    "\n",
    "        with self.keyword_pickle.open(\"rb\") as f:\n",
    "            raw_terms = pickle.load(f)\n",
    "\n",
    "        terms = [str(t).strip().lower() for t in list(raw_terms)]\n",
    "        if not terms:\n",
    "            raise ValueError(f\"No terms found in {self.keyword_pickle}\")\n",
    "\n",
    "        pattern = r\"\\b(\" + \"|\".join(re.escape(t) for t in terms) + r\")\\b\"\n",
    "        self.regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "    def predict(self, doc: Document) -> DocResult:\n",
    "        text = doc.text or \"\"\n",
    "        matches = self.regex.findall(text)\n",
    "        if not matches:\n",
    "            # no matches → score 0\n",
    "            span = Span(start=0, end=0, type=\"aapi_keyword\", score=0.0)\n",
    "            return DocResult(doc=doc, spans=[span])\n",
    "\n",
    "        # unique matches → score is count of unique AAPI terms present\n",
    "        unique_matches = {m.lower() for m in matches}\n",
    "        score = float(len(unique_matches))\n",
    "\n",
    "        span = Span(\n",
    "            start=0,\n",
    "            end=len(text),\n",
    "            type=\"aapi_keyword\",\n",
    "            score=score,\n",
    "        )\n",
    "        return DocResult(doc=doc, spans=[span])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14210657",
   "metadata": {},
   "source": [
    "### Dolma Mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c04aa1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_aapi_doc(result: DocResult) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Given a DocResult from AAPIKeywordsTagger, return a JSON-serializable dict\n",
    "    if score > 0, else return None (filter out the doc).\n",
    "    \"\"\"\n",
    "    doc = result.doc\n",
    "    spans = result.spans or []\n",
    "\n",
    "    score = 0.0\n",
    "    if spans:\n",
    "        score = float(spans[0].score or 0.0)\n",
    "\n",
    "    if score <= 0.0:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"id\": doc.id,\n",
    "        \"text\": doc.text,\n",
    "        \"source\": getattr(doc, \"source\", None),\n",
    "        \"aapi_score\": score,\n",
    "        \"aapi_spans\": [\n",
    "            {\n",
    "                \"start\": s.start,\n",
    "                \"end\": s.end,\n",
    "                \"type\": s.type,\n",
    "            }\n",
    "            for s in spans\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5730ef",
   "metadata": {},
   "source": [
    "### Process Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e35467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(count: int) -> None:\n",
    "    \"\"\"\n",
    "    Save how many C4 docs we've fully traversed/processed (for resume).\n",
    "    \"\"\"\n",
    "    PROGRESS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with PROGRESS_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"c4_docs_done\": count}, f)\n",
    "\n",
    "\n",
    "def load_progress() -> int:\n",
    "    \"\"\"\n",
    "    Load how many C4 docs we've previously processed. Returns 0 if none.\n",
    "    \"\"\"\n",
    "    if PROGRESS_PATH.exists():\n",
    "        with PROGRESS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return int(data.get(\"c4_docs_done\", 0))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def open_new_shard(out_dir: Path, shard_idx: int) -> gzip.GzipFile:\n",
    "    \"\"\"\n",
    "    Open a new gzip'd JSONL shard for writing mixed docs.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    shard_path = out_dir / f\"mixed.{shard_idx:09d}.jsonl.gz\"\n",
    "    return gzip.open(shard_path, \"wt\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc2382",
   "metadata": {},
   "source": [
    "### Actual Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2d0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loop(MAX_DOCS: int = 10_000, out_dirname: Path = OUTPUT_MIXED_DIR) -> None:\n",
    "    \"\"\"\n",
    "    Stream Dolma C4 docs, tag them with AAPIKeywordsTagger, filter them via\n",
    "    mix_aapi_doc, and write the kept ones into sharded .jsonl.gz files.\n",
    "\n",
    "    Resumable via PROGRESS_PATH (counts how many C4 docs have been processed).\n",
    "    \"\"\"\n",
    "    pre_computed = load_progress()\n",
    "    print(f\"Resuming from {pre_computed} pre-computed C4 documents.\")\n",
    "\n",
    "    kept = 0\n",
    "    traversed = 0\n",
    "    processed = pre_computed\n",
    "\n",
    "    tagger = AAPIKeywordsTagger()\n",
    "    ds = load_dataset(\n",
    "        \"allenai/dolma\",\n",
    "        \"v1_7\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    shard_idx = 0\n",
    "    docs_in_shard = 0\n",
    "    out_f: Optional[gzip.GzipFile] = None\n",
    "\n",
    "    pbar = tqdm(\n",
    "        total=MAX_DOCS,\n",
    "        initial=0,\n",
    "        desc=\"Processing C4 docs\",\n",
    "    )\n",
    "\n",
    "    actual_shard_size = min(SHARD_SIZE, MAX_DOCS)\n",
    "    for data in ds:\n",
    "        if data.get(\"source\") != \"c4\":\n",
    "            continue\n",
    "\n",
    "        if traversed < pre_computed:\n",
    "            traversed += 1\n",
    "            continue\n",
    "\n",
    "        doc = Document(\n",
    "            id=data[\"id\"],\n",
    "            text=data[\"text\"],\n",
    "            source=data.get(\"source\"),\n",
    "        )\n",
    "\n",
    "        tagged = tagger.predict(doc)\n",
    "        mixed = mix_aapi_doc(tagged)\n",
    "\n",
    "        traversed += 1\n",
    "        processed += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if mixed is not None:\n",
    "            if out_f is None or docs_in_shard >= actual_shard_size:\n",
    "                if out_f is not None:\n",
    "                    out_f.close()\n",
    "                out_f = open_new_shard(out_dirname, shard_idx)\n",
    "                shard_idx += 1\n",
    "                docs_in_shard = 0\n",
    "\n",
    "            out_f.write(json.dumps(mixed, ensure_ascii=False) + \"\\n\")\n",
    "            docs_in_shard += 1\n",
    "            kept += 1\n",
    "            \n",
    "\n",
    "        if processed % 1000 == 0:\n",
    "            save_progress(traversed)\n",
    "        if MAX_DOCS and docs_in_shard >= MAX_DOCS:\n",
    "            break\n",
    "\n",
    "    pbar.close()\n",
    "    if out_f is not None:\n",
    "        out_f.close()\n",
    "\n",
    "    save_progress(traversed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd23b43",
   "metadata": {},
   "source": [
    "### Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0092f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from 0 pre-computed C4 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs:   0%|          | 300/40000000 [13:31<16774:31:04,  1.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening new shard: data/output/mixed/mixed.000000000.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs:   2%|▏         | 941898/40000000 [34:01<7:30:08, 1446.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening new shard: data/output/mixed/mixed.000000001.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs:   5%|▍         | 1867478/40000000 [1:36:48<5:19:56, 1986.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening new shard: data/output/mixed/mixed.000000002.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs:   7%|▋         | 2799176/40000000 [2:35:35<7:01:40, 1470.37it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening new shard: data/output/mixed/mixed.000000003.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs:   9%|▉         | 3730963/40000000 [3:21:21<5:16:20, 1910.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening new shard: data/output/mixed/mixed.000000004.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs:  12%|█▏        | 4658978/40000000 [4:01:06<5:33:35, 1765.71it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening new shard: data/output/mixed/mixed.000000005.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs:  13%|█▎        | 5277929/40000000 [4:38:47<5:09:24, 1870.37it/s]  Got disconnected from remote data host. Retrying in 5sec [1/20]\n",
      "Processing C4 docs:  14%|█▍        | 5597273/40000000 [5:07:07<6:36:24, 1446.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening new shard: data/output/mixed/mixed.000000006.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs:  16%|█▋        | 6530375/40000000 [6:43:22<1162:24:54,  8.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening new shard: data/output/mixed/mixed.000000007.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs:  19%|█▊        | 7456856/40000000 [6:56:04<6:36:25, 1368.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening new shard: data/output/mixed/mixed.000000008.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs:  20%|█▉        | 7936651/40000000 [7:23:24<4:41:24, 1898.95it/s] Got disconnected from remote data host. Retrying in 5sec [1/20]\n",
      "Processing C4 docs:  21%|██        | 8387163/40000000 [8:12:37<4:51:06, 1809.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening new shard: data/output/mixed/mixed.000000009.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs:  23%|██▎       | 9322062/40000000 [8:46:24<4:17:38, 1984.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening new shard: data/output/mixed/mixed.000000010.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C4 docs:  25%|██▌       | 10030754/40000000 [8:55:13<5:32:32, 1502.04it/s]"
     ]
    }
   ],
   "source": [
    "# Creates 100 files of SHARD_SIZE each\n",
    "run_loop(SHARD_SIZE * 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ed3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1255d771",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolma-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
